{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25349,"status":"ok","timestamp":1677690919002,"user":{"displayName":"Fresh D","userId":"07909311957102528637"},"user_tz":480},"id":"0cnoLYlCRlxi","outputId":"55a4073a-64cc-4d6b-efb2-caff3739fde4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: transformers in /opt/anaconda/envs/transformers/lib/python3.8/site-packages (2.8.0)\n","Requirement already satisfied: scikit-learn in /opt/anaconda/envs/transformers/lib/python3.8/site-packages (0.23.2)\n","Requirement already satisfied: sentencepiece in /opt/anaconda/envs/transformers/lib/python3.8/site-packages (0.1.91)\n","Collecting datasets\n","  Using cached datasets-2.10.1-py3-none-any.whl (469 kB)\n","Requirement already satisfied: numpy>=1.17 in /opt/anaconda/envs/transformers/lib/python3.8/site-packages (from datasets) (1.19.4)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda/envs/transformers/lib/python3.8/site-packages (from datasets) (6.0)\n","Requirement already satisfied: packaging in ./.local/lib/python3.8/site-packages (from datasets) (23.0)\n","Requirement already satisfied: pyarrow>=6.0.0 in /opt/anaconda/envs/transformers/lib/python3.8/site-packages (from datasets) (8.0.0)\n","Requirement already satisfied: requests>=2.19.0 in /opt/anaconda/envs/transformers/lib/python3.8/site-packages (from datasets) (2.25.1)\n","Requirement already satisfied: pandas in /opt/anaconda/envs/transformers/lib/python3.8/site-packages (from datasets) (1.1.3)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/anaconda/envs/transformers/lib/python3.8/site-packages (from datasets) (2022.3.0)\n","Collecting dill<0.3.7,>=0.3.0\n","  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n","Collecting huggingface-hub<1.0.0,>=0.2.0\n","  Using cached huggingface_hub-0.13.0-py3-none-any.whl (199 kB)\n","Requirement already satisfied: filelock in /opt/anaconda/envs/transformers/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.0.12)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda/envs/transformers/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.1.1)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /opt/anaconda/envs/transformers/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda/envs/transformers/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda/envs/transformers/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.2)\n","Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda/envs/transformers/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.10)\n","Collecting responses<0.19\n","  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n","Collecting tqdm>=4.62.1\n","  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 6.2 MB/s  eta 0:00:01\n","\u001b[?25hCollecting scikit-learn\n","  Downloading scikit_learn-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n","\u001b[K     |████████████████████████████████| 9.8 MB 22.7 MB/s eta 0:00:01\n","\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /opt/anaconda/envs/transformers/lib/python3.8/site-packages (from scikit-learn) (1.5.4)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda/envs/transformers/lib/python3.8/site-packages (from scikit-learn) (2.1.0)\n","Collecting joblib>=1.1.1\n","  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n","\u001b[K     |████████████████████████████████| 297 kB 43.6 MB/s eta 0:00:01\n","\u001b[?25hCollecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 84.0 MB/s eta 0:00:01\n","\u001b[?25hCollecting transformers\n","  Using cached transformers-4.26.1-py3-none-any.whl (6.3 MB)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda/envs/transformers/lib/python3.8/site-packages (from transformers) (2020.11.13)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 43.2 MB/s eta 0:00:01\n","\u001b[?25hCollecting aiohttp\n","  Downloading aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[K     |████████████████████████████████| 1.0 MB 43.4 MB/s eta 0:00:01\n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting attrs>=17.3.0\n","  Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\n","\u001b[K     |████████████████████████████████| 60 kB 12.3 MB/s eta 0:00:01\n","\u001b[?25hCollecting charset-normalizer<4.0,>=2.0\n","  Downloading charset_normalizer-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (195 kB)\n","\u001b[K     |████████████████████████████████| 195 kB 87.0 MB/s eta 0:00:01\n","\u001b[?25hCollecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n","\u001b[K     |████████████████████████████████| 161 kB 109.4 MB/s eta 0:00:01\n","\u001b[?25hCollecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n","\u001b[K     |████████████████████████████████| 121 kB 99.4 MB/s eta 0:00:01\n","\u001b[?25hCollecting yarl<2.0,>=1.0\n","  Downloading yarl-1.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\n","\u001b[K     |████████████████████████████████| 262 kB 106.2 MB/s eta 0:00:01\n","\u001b[?25hCollecting multiprocess\n","  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 112.4 MB/s eta 0:00:01\n","\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /opt/anaconda/envs/transformers/lib/python3.8/site-packages (from pandas->datasets) (2020.4)\n","Requirement already satisfied: python-dateutil>=2.7.3 in ./.local/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /opt/anaconda/envs/transformers/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Collecting xxhash\n","  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n","\u001b[K     |████████████████████████████████| 213 kB 83.5 MB/s eta 0:00:01\n","\u001b[?25hInstalling collected packages: multidict, frozenlist, yarl, charset-normalizer, attrs, async-timeout, aiosignal, tqdm, dill, aiohttp, xxhash, tokenizers, responses, multiprocess, joblib, huggingface-hub, transformers, sentencepiece, scikit-learn, datasets\n","\u001b[33m  WARNING: The script normalizer is installed in '/home/deimann/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n","\u001b[33m  WARNING: The script tqdm is installed in '/home/deimann/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n","\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/deimann/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n","\u001b[33m  WARNING: The script transformers-cli is installed in '/home/deimann/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n","\u001b[33m  WARNING: The script datasets-cli is installed in '/home/deimann/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","thinc 7.4.3 requires blis<0.8.0,>=0.4.0, which is not installed.\n","thinc 7.4.3 requires cymem<2.1.0,>=2.0.2, which is not installed.\n","thinc 7.4.3 requires murmurhash<1.1.0,>=0.28.0, which is not installed.\n","thinc 7.4.3 requires plac<1.2.0,>=0.9.6, which is not installed.\n","thinc 7.4.3 requires preshed<3.1.0,>=1.0.1, which is not installed.\n","thinc 7.4.3 requires wasabi<1.1.0,>=0.0.9, which is not installed.\n","spacy 2.3.4 requires blis<0.8.0,>=0.4.0; python_version >= \"3.6\", which is not installed.\n","spacy 2.3.4 requires cymem<2.1.0,>=2.0.2, which is not installed.\n","spacy 2.3.4 requires murmurhash<1.1.0,>=0.28.0, which is not installed.\n","spacy 2.3.4 requires plac<1.2.0,>=0.9.6, which is not installed.\n","spacy 2.3.4 requires preshed<3.1.0,>=3.0.2, which is not installed.\n","spacy 2.3.4 requires wasabi<1.1.0,>=0.4.0, which is not installed.\n","spacy-transformers 0.6.2 requires ftfy<6.0.0,>=5.0.0, which is not installed.\n","spacy-transformers 0.6.2 requires torchcontrib<0.1.0,>=0.0.2, which is not installed.\n","spacy-transformers 0.6.2 requires transformers<2.9.0,>=2.4.0, but you have transformers 4.26.1 which is incompatible.\u001b[0m\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 attrs-22.2.0 charset-normalizer-3.1.0 datasets-2.10.1 dill-0.3.6 frozenlist-1.3.3 huggingface-hub-0.13.0 joblib-1.2.0 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 scikit-learn-1.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 tqdm-4.65.0 transformers-4.26.1 xxhash-3.2.0 yarl-1.8.2\n"]}],"source":["!pip install -U transformers datasets scikit-learn sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40843,"status":"ok","timestamp":1677690860876,"user":{"displayName":"Fresh D","userId":"07909311957102528637"},"user_tz":480},"id":"IrXbCW25igzD","outputId":"78b14933-714b-4a12-e76b-b0a63b16dd05"},"outputs":[{"name":"stdout","output_type":"stream","text":["  labels                                           comments\n","0   ENTP  I think Si inferior is characterized by discon...\n","1   ENTP  Berating? Its not like this picture of an age-...\n","2   ENTP  Without questioning anything youre saying, may...\n","3   ENTP  It makes sense for a community to value utilit...\n","4   ENTP  Trying to get laid maybe? Ive done charitable ...\n"]}],"source":["import pandas as pd \n","\n","df = pd.read_feather(\"//media/data/mbti-reddit/preprocessed_df_new.feather\") #change this to proper path\n","#'/content/drive/MyDrive/Colab Notebooks/clickbait_hold_X.csv'\n","df=df.drop(columns=['authors','subreddit'])\n","\n","print(df.head())"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":767,"status":"ok","timestamp":1677690882812,"user":{"displayName":"Fresh D","userId":"07909311957102528637"},"user_tz":480},"id":"wNb8QVKzgUY2","outputId":"73900868-c906-4455-b190-1d3b2eb9052d"},"outputs":[{"name":"stdout","output_type":"stream","text":["        labels                                           comments\n","686284    INFP  Truth is, there will be always someone who's c...\n","2023079   INTP  Yeah, he's supposed to carefully plan out his ...\n","2042692   ISTJ  I was diagnosed before I found Reddit but I sp...\n","3098768   ENTP  But I’m also asking kung may parusa kay owner ...\n","1224160   INTP  That is a good start. Pay your debts on time, ...\n","         labels                                           comments\n","686284        4  Truth is, there will be always someone who's c...\n","2023079       0  Yeah, he's supposed to carefully plan out his ...\n","2042692       9  I was diagnosed before I found Reddit but I sp...\n","3098768       2  But I’m also asking kung may parusa kay owner ...\n","1224160       0  That is a good start. Pay your debts on time, ...\n","         labels                                               text\n","686284        4  Truth is, there will be always someone who's c...\n","2023079       0  Yeah, he's supposed to carefully plan out his ...\n","2042692       9  I was diagnosed before I found Reddit but I sp...\n","3098768       2  But I’m also asking kung may parusa kay owner ...\n","1224160       0  That is a good start. Pay your debts on time, ...\n"]}],"source":["df=df.sample(20000, random_state=1) #random sampling\n","print(df.head())\n","df['labels'] = df['labels'].replace(['INTP','ISTP','ENTP','ESTP','INFP','ISFP','ENFP','ESFP', \\\n","                                     'INTJ','ISTJ','ENTJ','ESTJ','INFJ','ISFJ','ENFJ','ESFJ'], \\\n","                                    [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]) \n","print(df.head())\n","df=df.rename(columns={'labels':'labels','comments':'text'})\n","print(df.head())"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2194,"status":"ok","timestamp":1677691039189,"user":{"displayName":"Fresh D","userId":"07909311957102528637"},"user_tz":480},"id":"mw0LKrwhJedT"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/deimann/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from datasets import Dataset\n","\n","dataset = Dataset.from_pandas(df)\n","dataset.shuffle(seed=27)\n","split_set = dataset.train_test_split(test_size=0.2)"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":518,"status":"ok","timestamp":1677663775157,"user":{"displayName":"Fresh D","userId":"07909311957102528637"},"user_tz":480},"id":"4Yt0VdaOVpix","outputId":"4a2b1bb6-5aa1-4e68-db7b-e773e85e421a"},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['labels', 'text', '__index_level_0__'],\n","    num_rows: 20000\n","})"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["dataset"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1677663777133,"user":{"displayName":"Fresh D","userId":"07909311957102528637"},"user_tz":480},"id":"SQtUnJZjMbsK","outputId":"9709c507-708e-4c99-9f85-03332a169a91"},"outputs":[{"data":{"text/plain":["datasets.dataset_dict.DatasetDict"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["type(split_set)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220,"referenced_widgets":["d6cea55ac95d41cab14ea0f041ee2ebd","14286324014b4871ad1f1b5b75ed335c","a10266815856446983ea0ca419822040","e05066b4f0d442a89331595f7af06a6b","4491fc4889484d0e9159cd6cc89c17c8","dec4e9b1472b40d6ac896dd3abcb19dc","f57d582e3cfa4bde89da78898466720e","9ae4baf4ddea445f86d290c4ba48caf2","42a9253f91e5495b91d7552c17e31eef","51e43b1b365047d1be2f40c9fe616bd6","41f60bba1f3c441bb75c849b73684def","04b03326a58c4b04959106191333891b","4344b3a2edb94896b0cd80e951f3332b","e7f313ed364643ff89603ec9a4f939a6","fc455db41b0a43ca87d96c92d133f8f8","1aa65aa8386747ff947c9c622b9a483b","a9d7715172664b2caf6567507cc6e98e","e6f963e1e1654ae69a5e7d06aa2a937b","3b650c1d84624a958e786f05c2137caa","36d71624202642e7b92eb512d9e29454","4c7becc8fe714f148792c0352961188b","2063dae8b53b427e9cf36863002f3ba7","ea66f1be5a2f4327be3dec04b4613de7","f3fe2adff1584bc880e2830bf3827c3a","6b10e53bf99b4c22a5a52fd38373d6ec","67dc88eb5bcb4f20b1c9c90600823593","79e91372a055446b9df49131652c22cd","1b76972b7b1e435daa5c4841e2c8aedf","c777ebf42304423fa8e7bed77b3afcce","51534136256e48a9be4c7d7f33a90595","0c132c920f294f35bd707b186a564b31","e4f20c06f6d047cc89015f45b8f2e1c5","c37fb997fe8c4039afcc759f9b2aed68"]},"executionInfo":{"elapsed":16850,"status":"ok","timestamp":1677691062784,"user":{"displayName":"Fresh D","userId":"07909311957102528637"},"user_tz":480},"id":"hB0JoQXSlqp5","outputId":"6d773340-f8de-4cd5-af3a-fe42c0ce25d3"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.bias']\n","- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import AlbertTokenizer, AlbertModel\n","\n","tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n","\n","from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\"albert-base-v2\", num_labels=16)\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17,"referenced_widgets":["c2814328081e40f1bfa7f1608dd9886e","2271b7abac614e1c8bcb14d4241cd826","99b8d0213e7c4500977c1e257611781c","5f373bbfef514d0cac8a6ee21127fe85","555fe069bc9940f18c3bd360d7338ac3","7cf94f5bf52e45d99a6c1036afc84109","a69b599e15a84247ad78b83e2ec26f87","6076009bf06340eea434557b042c5d27","935a3bae1a184593ad9ac54a0622a0d4","8fdcaf475d2441babe02e3613e532669","15912ea12eac4119bf0d9dd0f3aefb3e","2d3e5b6b2e63404996e033d3e5a9f3e4","1bfd5da938a34e9585cea3e8431fa66f","4865f37f0d1e44d2a05e693a306222c7","4c9c1af440704e099e35d648fc45337b","74a3f350c90840a1b87d5d9ab415eb69","15e800d4d34b46c7825fa6f8b9dca3db","175df6de0f8c48b09110042efefe953b","617d744513f94bac96f800b0454d7afc","38c251559f6449d99a82f9d786f99a58","594bc8f718e74117a8f85a052af036ae","f39ddad88de8470d9c948bd5a731e1d4"]},"executionInfo":{"elapsed":28096,"status":"ok","timestamp":1677691092975,"user":{"displayName":"Fresh D","userId":"07909311957102528637"},"user_tz":480},"id":"4UIdGZIL-d7H","outputId":"a803c3f8-b52f-46d4-9a14-73688bc8faa1"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                   \r"]}],"source":["def preprocess_function(examples):\n","    return tokenizer(examples[\"text\"], truncation=True)\n","\n","tokenized_dataset = split_set.map(preprocess_function, batched=True)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1677691092978,"user":{"displayName":"Fresh D","userId":"07909311957102528637"},"user_tz":480},"id":"AqgLA4DbmDhb"},"outputs":[],"source":["from transformers import DataCollatorWithPadding\n","#tokenized_datasets = tokenized_datasets.remove_columns(books_dataset[\"train\"].column_names)\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":335,"status":"ok","timestamp":1677663822761,"user":{"displayName":"Fresh D","userId":"07909311957102528637"},"user_tz":480},"id":"Fme4O-PvD3gS","outputId":"498f01ef-1db7-4144-8a0b-a9a06f0caedf"},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['labels', 'text', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n","        num_rows: 16000\n","    })\n","    test: Dataset({\n","        features: ['labels', 'text', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n","        num_rows: 4000\n","    })\n","})\n"]}],"source":["print(tokenized_dataset)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["CUDA_VISIBLE_DEVICES = 1,2"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":3980343,"status":"ok","timestamp":1677668462609,"user":{"displayName":"Fresh D","userId":"07909311957102528637"},"user_tz":480},"id":"5O3xHCfcmIvI","outputId":"af74f571-b2b1-4b8e-d31d-4d0406e7f951"},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the training set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n","/home/deimann/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","/opt/anaconda/envs/transformers/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:30: UserWarning: \n","    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n","    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n","    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n","    environment variable.\n","  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n","***** Running training *****\n","  Num examples = 16000\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 96\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 167\n","  Number of trainable parameters = 11695888\n","/opt/anaconda/envs/transformers/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2' max='167' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [  2/167 : < :, Epoch 0.01/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"ename":"RuntimeError","evalue":"Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/opt/anaconda/envs/transformers/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/anaconda/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/deimann/.local/lib/python3.8/site-packages/transformers/models/albert/modeling_albert.py\", line 1076, in forward\n    outputs = self.albert(\n  File \"/opt/anaconda/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/deimann/.local/lib/python3.8/site-packages/transformers/models/albert/modeling_albert.py\", line 731, in forward\n    encoder_outputs = self.encoder(\n  File \"/opt/anaconda/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/deimann/.local/lib/python3.8/site-packages/transformers/models/albert/modeling_albert.py\", line 479, in forward\n    layer_group_output = self.albert_layer_groups[group_idx](\n  File \"/opt/anaconda/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/deimann/.local/lib/python3.8/site-packages/transformers/models/albert/modeling_albert.py\", line 431, in forward\n    layer_output = albert_layer(hidden_states, attention_mask, head_mask[layer_index], output_attentions)\n  File \"/opt/anaconda/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/deimann/.local/lib/python3.8/site-packages/transformers/models/albert/modeling_albert.py\", line 394, in forward\n    attention_output = self.attention(hidden_states, attention_mask, head_mask, output_attentions)\n  File \"/opt/anaconda/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/deimann/.local/lib/python3.8/site-packages/transformers/models/albert/modeling_albert.py\", line 329, in forward\n    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\nRuntimeError: CUDA out of memory. Tried to allocate 140.00 MiB (GPU 1; 10.76 GiB total capacity; 9.34 GiB already allocated; 32.56 MiB free; 9.83 GiB reserved in total by PyTorch)\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 33\u001b[0m\n\u001b[1;32m      1\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/home/deimann/mbti-project\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m )\n\u001b[0;32m---> 33\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:1543\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1540\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1541\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1542\u001b[0m )\n\u001b[0;32m-> 1543\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1544\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1545\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1546\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1547\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1548\u001b[0m )\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:1791\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1789\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1790\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1791\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1793\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1794\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1795\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1796\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1797\u001b[0m ):\n\u001b[1;32m   1798\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1799\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:2539\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2536\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2538\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2539\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2541\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2542\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:2571\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2569\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2570\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2571\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2572\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2573\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2574\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n","File \u001b[0;32m/opt/anaconda/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n","File \u001b[0;32m/opt/anaconda/envs/transformers/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:161\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs[\u001b[39m0\u001b[39m])\n\u001b[1;32m    160\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 161\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparallel_apply(replicas, inputs, kwargs)\n\u001b[1;32m    162\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgather(outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_device)\n","File \u001b[0;32m/opt/anaconda/envs/transformers/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:171\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparallel_apply\u001b[39m(\u001b[39mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 171\u001b[0m     \u001b[39mreturn\u001b[39;00m parallel_apply(replicas, inputs, kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice_ids[:\u001b[39mlen\u001b[39;49m(replicas)])\n","File \u001b[0;32m/opt/anaconda/envs/transformers/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:86\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     84\u001b[0m     output \u001b[39m=\u001b[39m results[i]\n\u001b[1;32m     85\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m---> 86\u001b[0m         output\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m     87\u001b[0m     outputs\u001b[39m.\u001b[39mappend(output)\n\u001b[1;32m     88\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/opt/anaconda/envs/transformers/lib/python3.8/site-packages/torch/_utils.py:428\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexc_type, \u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    425\u001b[0m     \u001b[39m# Some exceptions have first argument as non-str but explicitly\u001b[39;00m\n\u001b[1;32m    426\u001b[0m     \u001b[39m# have message field\u001b[39;00m\n\u001b[1;32m    427\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexc_type(message\u001b[39m=\u001b[39mmsg)\n\u001b[0;32m--> 428\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexc_type(msg)\n","\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/opt/anaconda/envs/transformers/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/anaconda/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/deimann/.local/lib/python3.8/site-packages/transformers/models/albert/modeling_albert.py\", line 1076, in forward\n    outputs = self.albert(\n  File \"/opt/anaconda/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/deimann/.local/lib/python3.8/site-packages/transformers/models/albert/modeling_albert.py\", line 731, in forward\n    encoder_outputs = self.encoder(\n  File \"/opt/anaconda/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/deimann/.local/lib/python3.8/site-packages/transformers/models/albert/modeling_albert.py\", line 479, in forward\n    layer_group_output = self.albert_layer_groups[group_idx](\n  File \"/opt/anaconda/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/deimann/.local/lib/python3.8/site-packages/transformers/models/albert/modeling_albert.py\", line 431, in forward\n    layer_output = albert_layer(hidden_states, attention_mask, head_mask[layer_index], output_attentions)\n  File \"/opt/anaconda/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/deimann/.local/lib/python3.8/site-packages/transformers/models/albert/modeling_albert.py\", line 394, in forward\n    attention_output = self.attention(hidden_states, attention_mask, head_mask, output_attentions)\n  File \"/opt/anaconda/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/deimann/.local/lib/python3.8/site-packages/transformers/models/albert/modeling_albert.py\", line 329, in forward\n    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\nRuntimeError: CUDA out of memory. Tried to allocate 140.00 MiB (GPU 1; 10.76 GiB total capacity; 9.34 GiB already allocated; 32.56 MiB free; 9.83 GiB reserved in total by PyTorch)\n"]}],"source":["training_args = TrainingArguments(\n","\n","    output_dir=\"/home/deimann/mbti-project\",\n","\n","    learning_rate=2e-5,#2e\n","\n","    per_device_train_batch_size=16,#16\n","\n","    per_device_eval_batch_size=16,#16\n","\n","    num_train_epochs=1,\n","\n","    weight_decay=0.01,\n","\n",")\n","\n","trainer = Trainer(\n","\n","    model=model,\n","\n","    args=training_args,\n","\n","    train_dataset=tokenized_dataset[\"train\"],\n","\n","    eval_dataset=tokenized_dataset[\"test\"],\n","\n","    tokenizer=tokenizer,\n","\n","    data_collator=data_collator,\n","\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w8RYkK-RNMiO"},"outputs":[],"source":["device = cuda.get_current_device() \n","device.reset()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Thu Mar  9 16:46:04 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-PCIE...  On   | 00000000:1B:00.0 Off |                    0 |\n","| N/A   62C    P0    98W / 250W |  30091MiB / 32510MiB |     59%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","|   1  GeForce RTX 208...  On   | 00000000:1C:00.0 Off |                  N/A |\n","| 30%   30C    P2    47W / 250W |  10926MiB / 11019MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","|   2  GeForce RTX 208...  On   | 00000000:1E:00.0 Off |                  N/A |\n","| 30%   33C    P2    52W / 250W |  10926MiB / 11019MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","|   3  GeForce RTX 208...  On   | 00000000:3D:00.0 Off |                  N/A |\n","| 30%   30C    P2    54W / 250W |  10926MiB / 11019MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","|   4  GeForce RTX 208...  On   | 00000000:3F:00.0 Off |                  N/A |\n","| 30%   31C    P2    45W / 250W |  10929MiB / 11019MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","|   5  GeForce RTX 208...  On   | 00000000:41:00.0 Off |                  N/A |\n","| 30%   31C    P2    56W / 250W |  10929MiB / 11019MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|    0   N/A  N/A   3010134      C   /opt/anaconda/bin/python         6665MiB |\n","|    0   N/A  N/A   3012061      C   ...s/transformers/bin/python    17637MiB |\n","|    0   N/A  N/A   3018833      C   python3                          5783MiB |\n","|    1   N/A  N/A   3012061      C   ...s/transformers/bin/python    10923MiB |\n","|    2   N/A  N/A   3012061      C   ...s/transformers/bin/python    10923MiB |\n","|    3   N/A  N/A   3012061      C   ...s/transformers/bin/python    10923MiB |\n","|    4   N/A  N/A    300111      C   ...s/bert-service/bin/python     1309MiB |\n","|    4   N/A  N/A   3012061      C   ...s/transformers/bin/python     9615MiB |\n","|    5   N/A  N/A    300114      C   ...s/bert-service/bin/python     1309MiB |\n","|    5   N/A  N/A   3012061      C   ...s/transformers/bin/python     9615MiB |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["AB HIER EINFACH IGNORIEREN!!!\n","!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DuHH119ZnR8q"},"outputs":[],"source":["from transformers import TextClassificationPipeline\n","\n","pipeline = TextClassificationPipeline(model=model,\n","                                      tokenizer=tokenizer,\n","                                      device=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n7v5KorQNSAL"},"outputs":[],"source":["model_name_or_path=/content/models/checkpoint-5000"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["2c46f18b4ef04865b60fe1d92eeb3d88","6fcee0d969b740caad3095d8a5b46a85","f8e145ccf7bf4efda5e5736570abc032","53fe8fac6b0a466fb3024a9afe5589fd","89fbece0fe1b4f869172d3b57043c927","9601130f1f4041c4afe06ee46ac8384a","23ac9886084a490f94455e3f9987965b","2c282de0fe4949319c304e59f8751fb7","cc524b549026490dbc33391be5496ce8","536b9afe31344292a4349aaec8d277c7","fcd570f697fd41008a15580aeea7cea3"]},"executionInfo":{"elapsed":3972304,"status":"ok","timestamp":1677676543227,"user":{"displayName":"Fresh D","userId":"07909311957102528637"},"user_tz":480},"id":"UzvI5PVxEQGR","outputId":"4a84982c-4498-4751-9ddf-fab71dfc1ae6"},"outputs":[{"name":"stderr","output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","Loading model from /content/drive/MyDrive/Colab Notebooks/checkpoint-4000.\n","The following columns in the training set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 16000\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 20\n","  Total train batch size (w. parallel, distributed & accumulation) = 20\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 8000\n","  Number of trainable parameters = 11695888\n","  Continuing training from checkpoint, will skip to saved global_step\n","  Continuing training from epoch 5\n","  Continuing training from global step 4000\n","  Will skip the first 5 epochs then the first 0 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2c46f18b4ef04865b60fe1d92eeb3d88","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='8000' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [8000/8000 1:06:09, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>4500</td>\n","      <td>2.279100</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>2.241800</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>2.150100</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>2.006000</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>1.899500</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>1.728800</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>1.623900</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>1.543700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/checkpoint-4500\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-4500/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-4500/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-4500/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-4500/special_tokens_map.json\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/checkpoint-5000\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-5000/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-5000/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-5000/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-5000/special_tokens_map.json\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/checkpoint-5500\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-5500/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-5500/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-5500/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-5500/special_tokens_map.json\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/checkpoint-6000\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-6000/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-6000/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-6000/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-6000/special_tokens_map.json\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/checkpoint-6500\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-6500/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-6500/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-6500/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-6500/special_tokens_map.json\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/checkpoint-7000\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-7000/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-7000/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-7000/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-7000/special_tokens_map.json\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/checkpoint-7500\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-7500/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-7500/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-7500/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-7500/special_tokens_map.json\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/checkpoint-8000\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-8000/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-8000/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-8000/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-8000/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"data":{"text/plain":["TrainOutput(global_step=8000, training_loss=0.9670639953613281, metrics={'train_runtime': 3971.1481, 'train_samples_per_second': 40.291, 'train_steps_per_second': 2.015, 'total_flos': 2133879299719680.0, 'train_loss': 0.9670639953613281, 'epoch': 10.0})"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["training_args = TrainingArguments(\n","\n","    output_dir=\"/content/drive/MyDrive/Colab Notebooks/\",\n","\n","    learning_rate=2e-5,#2e\n","\n","    per_device_train_batch_size=20,#16\n","\n","    per_device_eval_batch_size=20,#16\n","\n","    num_train_epochs=10,\n","\n","    weight_decay=0.01,\n","\n",")\n","\n","trainer = Trainer(\n","\n","    model=model,\n","\n","    args=training_args,\n","\n","    train_dataset=tokenized_dataset[\"train\"],\n","\n","    eval_dataset=tokenized_dataset[\"test\"],\n","\n","    tokenizer=tokenizer,\n","\n","    data_collator=data_collator,\n","\n",")\n","\n","trainer.train(\"/content/drive/MyDrive/Colab Notebooks/checkpoint-4000\")"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["5c351c233a454fb3b529091aad32462f","a5ac51b065b049b093a9da19df83fff1","5307fb453aef4d8d908161163390dd0f","2e0d08738f5e40328a5250faf2f9b6be","5f10e42fc0e4499eba7d58efa592e16f","ec207f19217e4cd590d8fc519188097e","f0428ded257a4388a2f9c3160c1d6b66","c0249fe7980e40338fdacb10aa1c0c57","ac9a9b566a8b45d8a0fbba1b5c017e7c","25a1d7a57446445080fb947e8446dfa4","a127002e65734aeba275e237c203af63"]},"executionInfo":{"elapsed":3971241,"status":"ok","timestamp":1677681495972,"user":{"displayName":"Fresh D","userId":"07909311957102528637"},"user_tz":480},"id":"zR-Ma38sXyDS","outputId":"c55742ba-ae94-4857-e674-6e8865724b3c"},"outputs":[{"name":"stderr","output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","Loading model from /content/drive/MyDrive/Colab Notebooks/checkpoint-8000.\n","The following columns in the training set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 16000\n","  Num Epochs = 15\n","  Instantaneous batch size per device = 20\n","  Total train batch size (w. parallel, distributed & accumulation) = 20\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 12000\n","  Number of trainable parameters = 11695888\n","  Continuing training from checkpoint, will skip to saved global_step\n","  Continuing training from epoch 10\n","  Continuing training from global step 8000\n","  Will skip the first 10 epochs then the first 0 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5c351c233a454fb3b529091aad32462f","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='12000' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12000/12000 1:06:09, Epoch 15/15]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>8500</td>\n","      <td>1.532800</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>1.503300</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>1.413300</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>1.280600</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>1.208100</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>1.072200</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>0.994600</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>0.926500</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/checkpoint-8500\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-8500/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-8500/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-8500/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-8500/special_tokens_map.json\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/checkpoint-9000\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-9000/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-9000/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-9000/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-9000/special_tokens_map.json\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/checkpoint-9500\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-9500/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-9500/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-9500/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-9500/special_tokens_map.json\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/checkpoint-10000\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-10000/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-10000/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-10000/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-10000/special_tokens_map.json\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/checkpoint-10500\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-10500/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-10500/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-10500/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-10500/special_tokens_map.json\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/checkpoint-11000\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-11000/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-11000/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-11000/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-11000/special_tokens_map.json\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/checkpoint-11500\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-11500/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-11500/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-11500/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-11500/special_tokens_map.json\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/checkpoint-12000\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-12000/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-12000/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-12000/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/checkpoint-12000/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"data":{"text/plain":["TrainOutput(global_step=12000, training_loss=0.4138126703898112, metrics={'train_runtime': 3970.6209, 'train_samples_per_second': 60.444, 'train_steps_per_second': 3.022, 'total_flos': 3197273209484160.0, 'train_loss': 0.4138126703898112, 'epoch': 15.0})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["training_args = TrainingArguments(\n","\n","    output_dir=\"/content/drive/MyDrive/Colab Notebooks/\",\n","\n","    learning_rate=2e-5,#2e\n","\n","    per_device_train_batch_size=20,#16\n","\n","    per_device_eval_batch_size=20,#16\n","\n","    num_train_epochs=15,\n","\n","    weight_decay=0.01,\n","\n",")\n","\n","trainer = Trainer(\n","\n","    model=model,\n","\n","    args=training_args,\n","\n","    train_dataset=tokenized_dataset[\"train\"],\n","\n","    eval_dataset=tokenized_dataset[\"test\"],\n","\n","    tokenizer=tokenizer,\n","\n","    data_collator=data_collator,\n","\n",")\n","\n","trainer.train(\"/content/drive/MyDrive/Colab Notebooks/checkpoint-8000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F9cnPpqfiiWi"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"0k99hg-tl5wR"},"source":["OutOfMemoryError: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 14.75 GiB total capacity; 12.97 GiB already allocated; 62.81 MiB free; 13.90 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":2381,"status":"ok","timestamp":1677691249720,"user":{"displayName":"Fresh D","userId":"07909311957102528637"},"user_tz":480},"id":"PLBFet2PLtKQ"},"outputs":[],"source":["\n","model = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/checkpoint-12000\", num_labels=16)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":2990,"status":"ok","timestamp":1677691323771,"user":{"displayName":"Fresh D","userId":"07909311957102528637"},"user_tz":480},"id":"_5YgWZnSMb_M"},"outputs":[],"source":["from transformers import TextClassificationPipeline\n","\n","pipeline = TextClassificationPipeline(model=model,tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":166041,"status":"error","timestamp":1677691696997,"user":{"displayName":"Fresh D","userId":"07909311957102528637"},"user_tz":480},"id":"fQlPfxfqRY3h","outputId":"733aa2d6-fcee-4528-b429-ee2b6155ab30"},"outputs":[{"name":"stderr","output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (1240 > 512). Running this sequence through the model will result in indexing errors\n"]},{"ename":"RuntimeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-5f081bc21aba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mtrue_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mpredicted_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpredicted_label\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'LABEL_0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/text_classification.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone\u001b[0m \u001b[0msuch\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m \u001b[0mper\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \"\"\"\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;31m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0m_legacy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"top_k\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m             )\n\u001b[1;32m   1083\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    990\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/text_classification.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_to_apply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_legacy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/albert/modeling_albert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1074\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m         outputs = self.albert(\n\u001b[0m\u001b[1;32m   1077\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/albert/modeling_albert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m    729\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/albert/modeling_albert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"absolute\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             \u001b[0membeddings\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1240) must match the size of tensor b (512) at non-singleton dimension 1"]}],"source":["from sklearn.metrics import f1_score\n","\n","true_labels = []\n","predicted_labels = []\n","\n","for test in split_set[\"test\"]:\n","  true_labels.append(test[\"labels\"])\n","  predicted_label = pipeline(test[\"text\"])[0]['label']\n","  if predicted_label == 'LABEL_0':\n","    predicted_labels.append(0)\n","  elif predicted_label == 'LABEL_1':\n","    predicted_labels.append(1)\n","  elif predicted_label == 'LABEL_2':\n","    predicted_labels.append(2)\n","  elif predicted_label == 'LABEL_3':\n","    predicted_labels.append(3)\n","  elif predicted_label == 'LABEL_4':\n","    predicted_labels.append(4)\n","  elif predicted_label == 'LABEL_5':\n","    predicted_labels.append(5)\n","  elif predicted_label == 'LABEL_6':\n","    predicted_labels.append(6)\n","  elif predicted_label == 'LABEL_7':\n","    predicted_labels.append(7)\n","  elif predicted_label == 'LABEL_8':\n","    predicted_labels.append(8)\n","  elif predicted_label == 'LABEL_9':\n","    predicted_labels.append(9)\n","  elif predicted_label == 'LABEL_10':\n","    predicted_labels.append(10)\n","  elif predicted_label == 'LABEL_11':\n","    predicted_labels.append(11)\n","  elif predicted_label == 'LABEL_12':\n","    predicted_labels.append(12)\n","  elif predicted_label == 'LABEL_13':\n","    predicted_labels.append(13)\n","  elif predicted_label == 'LABEL_14':\n","    predicted_labels.append(14)\n","  elif predicted_label == 'LABEL_15':\n","    predicted_labels.append(15)\n","  else:\n","    print(\"unexpected label\")\n","    print(predicted_label)\n","  \n","  #to show the predicted labels see below\n","  #print(test[\"text\"])\n","  #print(test[\"label\"])\n","  #print()\n","\n","print(f1_score(true_labels, predicted_labels, average='binary'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9wtruY_hOOJD"},"outputs":[],"source":["smaller_split = np.sample()"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":111115,"status":"ok","timestamp":1677693628727,"user":{"displayName":"Fresh D","userId":"07909311957102528637"},"user_tz":480},"id":"1irZnWS5O4YH"},"outputs":[],"source":["from sklearn.metrics import f1_score\n","\n","true_labels = []\n","predicted_labels = []\n","a = 0\n","for test in split_set[\"test\"]:\n","  a+=1\n","  if a==300:\n","    break\n","  true_labels.append(test[\"labels\"])\n","  predicted_label = pipeline(test[\"text\"])[0]['label']\n","\n","  if predicted_label == 'LABEL_0':\n","    predicted_labels.append(0)\n","  elif predicted_label == 'LABEL_1':\n","    predicted_labels.append(1)\n","  elif predicted_label == 'LABEL_2':\n","    predicted_labels.append(2)\n","  elif predicted_label == 'LABEL_3':\n","    predicted_labels.append(3)\n","  elif predicted_label == 'LABEL_4':\n","    predicted_labels.append(4)\n","  elif predicted_label == 'LABEL_5':\n","    predicted_labels.append(5)\n","  elif predicted_label == 'LABEL_6':\n","    predicted_labels.append(6)\n","  elif predicted_label == 'LABEL_7':\n","    predicted_labels.append(7)\n","  elif predicted_label == 'LABEL_8':\n","    predicted_labels.append(8)\n","  elif predicted_label == 'LABEL_9':\n","    predicted_labels.append(9)\n","  elif predicted_label == 'LABEL_10':\n","    predicted_labels.append(10)\n","  elif predicted_label == 'LABEL_11':\n","    predicted_labels.append(11)\n","  elif predicted_label == 'LABEL_12':\n","    predicted_labels.append(12)\n","  elif predicted_label == 'LABEL_13':\n","    predicted_labels.append(13)\n","  elif predicted_label == 'LABEL_14':\n","    predicted_labels.append(14)\n","  elif predicted_label == 'LABEL_15':\n","    predicted_labels.append(15)\n","  else:\n","    print(\"unexpected label\")\n","    print(predicted_label)\n","  \n","  #to show the predicted labels see below\n","  #print(test[\"text\"])\n","  #print(test[\"labels\"])\n","  #print('predicted: '+pipeline(test[\"text\"])[0]['label'])\n","  #print()\n","\n","#print(f1_score(true_labels, predicted_labels, average=None))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VK45xW_lRIHZ"},"outputs":[],"source":["\n","### Confusion Matrix\n","from sklearn.metrics import confusion_matrix\n","predictions = model.predict(x_test, steps=len(x_test), verbose=0)\n","#y_pred=model.predict(x_test)\n","#y_pred = np.round(y_pred)\n","y_pred = np.argmax(predictions, axis=-1)\n","\n","y_true=np.argmax(y_test, axis=-1)\n","\n","cm = confusion_matrix(y_true, y_pred)\n","\n","## Get Class Labels\n","labels = le.classes_\n","class_names = labels\n","\n","# Plot confusion matrix in a beautiful manner\n","fig = plt.figure(figsize=(16, 14))\n","ax= plt.subplot()\n","sns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cells\n","# labels, title and ticks\n","ax.set_xlabel('Predicted', fontsize=20)\n","ax.xaxis.set_label_position('bottom')\n","plt.xticks(rotation=90)\n","ax.xaxis.set_ticklabels(class_names, fontsize = 10)\n","ax.xaxis.tick_bottom()\n","\n","ax.set_ylabel('True', fontsize=20)\n","ax.yaxis.set_ticklabels(class_names, fontsize = 10)\n","plt.yticks(rotation=0)\n","\n","plt.title('Refined Confusion Matrix', fontsize=20)\n","\n","plt.savefig('ConMat24.png')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uFEJjtfvUVxg"},"outputs":[],"source":["\n","\n","df['labels'] = df['labels'].replace(['INTP','ISTP','ENTP','ESTP','INFP','ISFP','ENFP','ESFP', \\\n","                                     'INTJ','ISTJ','ENTJ','ESTJ','INFJ','ISFJ','ENFJ','ESFJ'], \\\n","                                    [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]) "]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":487,"status":"ok","timestamp":1677693948914,"user":{"displayName":"Fresh D","userId":"07909311957102528637"},"user_tz":480},"id":"eiOctrmoVYMR","outputId":"f3d3b693-edf9-4bf8-b512-c6f2273516eb"},"outputs":[{"name":"stdout","output_type":"stream","text":["     true predicted\n","0    ENTP      INTJ\n","1    INFP      ENFP\n","2    ESTP      INFP\n","3    ISFP      ISTJ\n","4    INFJ      INFP\n","..    ...       ...\n","294  ISTP      INTJ\n","295  INFJ      INTP\n","296  INTP      INTP\n","297  INTP      INFP\n","298  INTJ      ENFP\n","\n","[299 rows x 2 columns]\n"]}],"source":["evaldf = pd.DataFrame(\n","    {'true': true_labels,\n","     'predicted': predicted_labels\n","    })\n","evaldf['true'] = evaldf['true'].replace([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15], \\\n","                                         ['INTP','ISTP','ENTP','ESTP','INFP','ISFP','ENFP','ESFP', \\\n","                                     'INTJ','ISTJ','ENTJ','ESTJ','INFJ','ISFJ','ENFJ','ESFJ'])\n","                                    \n","evaldf['predicted'] = evaldf['predicted'].replace([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15], \\\n","                                         ['INTP','ISTP','ENTP','ESTP','INFP','ISFP','ENFP','ESFP', \\\n","                                     'INTJ','ISTJ','ENTJ','ESTJ','INFJ','ISFJ','ENFJ','ESFJ'])\n","\n","print(evaldf)\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":167},"executionInfo":{"elapsed":407,"status":"error","timestamp":1677853556815,"user":{"displayName":"Fresh D","userId":"07909311957102528637"},"user_tz":480},"id":"nLfKOa143RZ4","outputId":"08d2c380-8c32-478a-d718-37ed5d4ccfa5"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-b7742239926d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaldf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'evaldf' is not defined"]}],"source":["print(evaldf[0])"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":842,"status":"ok","timestamp":1677694235371,"user":{"displayName":"Fresh D","userId":"07909311957102528637"},"user_tz":480},"id":"ZCxDJaJKW3-M"},"outputs":[],"source":["evaldf.to_csv(r'/content/drive/MyDrive/Colab Notebooks/evaldf1.csv', index=False)\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNQGaqVErhbgRcnvLa3TB8g","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"04b03326a58c4b04959106191333891b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4344b3a2edb94896b0cd80e951f3332b","IPY_MODEL_e7f313ed364643ff89603ec9a4f939a6","IPY_MODEL_fc455db41b0a43ca87d96c92d133f8f8"],"layout":"IPY_MODEL_1aa65aa8386747ff947c9c622b9a483b"}},"0c132c920f294f35bd707b186a564b31":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"14286324014b4871ad1f1b5b75ed335c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dec4e9b1472b40d6ac896dd3abcb19dc","placeholder":"​","style":"IPY_MODEL_f57d582e3cfa4bde89da78898466720e","value":"Downloading (…)ve/main/spiece.model: 100%"}},"15912ea12eac4119bf0d9dd0f3aefb3e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"15e800d4d34b46c7825fa6f8b9dca3db":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"175df6de0f8c48b09110042efefe953b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1aa65aa8386747ff947c9c622b9a483b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b76972b7b1e435daa5c4841e2c8aedf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1bfd5da938a34e9585cea3e8431fa66f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_15e800d4d34b46c7825fa6f8b9dca3db","placeholder":"​","style":"IPY_MODEL_175df6de0f8c48b09110042efefe953b","value":"Map: 100%"}},"2063dae8b53b427e9cf36863002f3ba7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2271b7abac614e1c8bcb14d4241cd826":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7cf94f5bf52e45d99a6c1036afc84109","placeholder":"​","style":"IPY_MODEL_a69b599e15a84247ad78b83e2ec26f87","value":"Map: 100%"}},"23ac9886084a490f94455e3f9987965b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25a1d7a57446445080fb947e8446dfa4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c282de0fe4949319c304e59f8751fb7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"2c46f18b4ef04865b60fe1d92eeb3d88":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6fcee0d969b740caad3095d8a5b46a85","IPY_MODEL_f8e145ccf7bf4efda5e5736570abc032","IPY_MODEL_53fe8fac6b0a466fb3024a9afe5589fd"],"layout":"IPY_MODEL_89fbece0fe1b4f869172d3b57043c927"}},"2d3e5b6b2e63404996e033d3e5a9f3e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1bfd5da938a34e9585cea3e8431fa66f","IPY_MODEL_4865f37f0d1e44d2a05e693a306222c7","IPY_MODEL_4c9c1af440704e099e35d648fc45337b"],"layout":"IPY_MODEL_74a3f350c90840a1b87d5d9ab415eb69"}},"2e0d08738f5e40328a5250faf2f9b6be":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_25a1d7a57446445080fb947e8446dfa4","placeholder":"​","style":"IPY_MODEL_a127002e65734aeba275e237c203af63","value":" 0/0 [00:00&lt;?, ?it/s]"}},"36d71624202642e7b92eb512d9e29454":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"38c251559f6449d99a82f9d786f99a58":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3b650c1d84624a958e786f05c2137caa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41f60bba1f3c441bb75c849b73684def":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"42a9253f91e5495b91d7552c17e31eef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4344b3a2edb94896b0cd80e951f3332b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9d7715172664b2caf6567507cc6e98e","placeholder":"​","style":"IPY_MODEL_e6f963e1e1654ae69a5e7d06aa2a937b","value":"Downloading (…)lve/main/config.json: 100%"}},"4491fc4889484d0e9159cd6cc89c17c8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4865f37f0d1e44d2a05e693a306222c7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_617d744513f94bac96f800b0454d7afc","max":4000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_38c251559f6449d99a82f9d786f99a58","value":4000}},"4c7becc8fe714f148792c0352961188b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c9c1af440704e099e35d648fc45337b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_594bc8f718e74117a8f85a052af036ae","placeholder":"​","style":"IPY_MODEL_f39ddad88de8470d9c948bd5a731e1d4","value":" 4000/4000 [00:06&lt;00:00, 662.28 examples/s]"}},"51534136256e48a9be4c7d7f33a90595":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51e43b1b365047d1be2f40c9fe616bd6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5307fb453aef4d8d908161163390dd0f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0249fe7980e40338fdacb10aa1c0c57","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ac9a9b566a8b45d8a0fbba1b5c017e7c","value":0}},"536b9afe31344292a4349aaec8d277c7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53fe8fac6b0a466fb3024a9afe5589fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_536b9afe31344292a4349aaec8d277c7","placeholder":"​","style":"IPY_MODEL_fcd570f697fd41008a15580aeea7cea3","value":" 0/0 [00:00&lt;?, ?it/s]"}},"555fe069bc9940f18c3bd360d7338ac3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"594bc8f718e74117a8f85a052af036ae":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c351c233a454fb3b529091aad32462f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a5ac51b065b049b093a9da19df83fff1","IPY_MODEL_5307fb453aef4d8d908161163390dd0f","IPY_MODEL_2e0d08738f5e40328a5250faf2f9b6be"],"layout":"IPY_MODEL_5f10e42fc0e4499eba7d58efa592e16f"}},"5f10e42fc0e4499eba7d58efa592e16f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f373bbfef514d0cac8a6ee21127fe85":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8fdcaf475d2441babe02e3613e532669","placeholder":"​","style":"IPY_MODEL_15912ea12eac4119bf0d9dd0f3aefb3e","value":" 16000/16000 [00:21&lt;00:00, 594.82 examples/s]"}},"6076009bf06340eea434557b042c5d27":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"617d744513f94bac96f800b0454d7afc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67dc88eb5bcb4f20b1c9c90600823593":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4f20c06f6d047cc89015f45b8f2e1c5","placeholder":"​","style":"IPY_MODEL_c37fb997fe8c4039afcc759f9b2aed68","value":" 47.4M/47.4M [00:00&lt;00:00, 128MB/s]"}},"6b10e53bf99b4c22a5a52fd38373d6ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_51534136256e48a9be4c7d7f33a90595","max":47376696,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0c132c920f294f35bd707b186a564b31","value":47376696}},"6fcee0d969b740caad3095d8a5b46a85":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9601130f1f4041c4afe06ee46ac8384a","placeholder":"​","style":"IPY_MODEL_23ac9886084a490f94455e3f9987965b","value":"Skipping the first batches: "}},"74a3f350c90840a1b87d5d9ab415eb69":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"79e91372a055446b9df49131652c22cd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cf94f5bf52e45d99a6c1036afc84109":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89fbece0fe1b4f869172d3b57043c927":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fdcaf475d2441babe02e3613e532669":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"935a3bae1a184593ad9ac54a0622a0d4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9601130f1f4041c4afe06ee46ac8384a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99b8d0213e7c4500977c1e257611781c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_6076009bf06340eea434557b042c5d27","max":16000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_935a3bae1a184593ad9ac54a0622a0d4","value":16000}},"9ae4baf4ddea445f86d290c4ba48caf2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a10266815856446983ea0ca419822040":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ae4baf4ddea445f86d290c4ba48caf2","max":760289,"min":0,"orientation":"horizontal","style":"IPY_MODEL_42a9253f91e5495b91d7552c17e31eef","value":760289}},"a127002e65734aeba275e237c203af63":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5ac51b065b049b093a9da19df83fff1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec207f19217e4cd590d8fc519188097e","placeholder":"​","style":"IPY_MODEL_f0428ded257a4388a2f9c3160c1d6b66","value":"Skipping the first batches: "}},"a69b599e15a84247ad78b83e2ec26f87":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a9d7715172664b2caf6567507cc6e98e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac9a9b566a8b45d8a0fbba1b5c017e7c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c0249fe7980e40338fdacb10aa1c0c57":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"c2814328081e40f1bfa7f1608dd9886e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2271b7abac614e1c8bcb14d4241cd826","IPY_MODEL_99b8d0213e7c4500977c1e257611781c","IPY_MODEL_5f373bbfef514d0cac8a6ee21127fe85"],"layout":"IPY_MODEL_555fe069bc9940f18c3bd360d7338ac3"}},"c37fb997fe8c4039afcc759f9b2aed68":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c777ebf42304423fa8e7bed77b3afcce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cc524b549026490dbc33391be5496ce8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d6cea55ac95d41cab14ea0f041ee2ebd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_14286324014b4871ad1f1b5b75ed335c","IPY_MODEL_a10266815856446983ea0ca419822040","IPY_MODEL_e05066b4f0d442a89331595f7af06a6b"],"layout":"IPY_MODEL_4491fc4889484d0e9159cd6cc89c17c8"}},"dec4e9b1472b40d6ac896dd3abcb19dc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e05066b4f0d442a89331595f7af06a6b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_51e43b1b365047d1be2f40c9fe616bd6","placeholder":"​","style":"IPY_MODEL_41f60bba1f3c441bb75c849b73684def","value":" 760k/760k [00:00&lt;00:00, 2.49MB/s]"}},"e4f20c06f6d047cc89015f45b8f2e1c5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6f963e1e1654ae69a5e7d06aa2a937b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e7f313ed364643ff89603ec9a4f939a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b650c1d84624a958e786f05c2137caa","max":684,"min":0,"orientation":"horizontal","style":"IPY_MODEL_36d71624202642e7b92eb512d9e29454","value":684}},"ea66f1be5a2f4327be3dec04b4613de7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f3fe2adff1584bc880e2830bf3827c3a","IPY_MODEL_6b10e53bf99b4c22a5a52fd38373d6ec","IPY_MODEL_67dc88eb5bcb4f20b1c9c90600823593"],"layout":"IPY_MODEL_79e91372a055446b9df49131652c22cd"}},"ec207f19217e4cd590d8fc519188097e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0428ded257a4388a2f9c3160c1d6b66":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f39ddad88de8470d9c948bd5a731e1d4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f3fe2adff1584bc880e2830bf3827c3a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b76972b7b1e435daa5c4841e2c8aedf","placeholder":"​","style":"IPY_MODEL_c777ebf42304423fa8e7bed77b3afcce","value":"Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"}},"f57d582e3cfa4bde89da78898466720e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f8e145ccf7bf4efda5e5736570abc032":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c282de0fe4949319c304e59f8751fb7","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cc524b549026490dbc33391be5496ce8","value":0}},"fc455db41b0a43ca87d96c92d133f8f8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c7becc8fe714f148792c0352961188b","placeholder":"​","style":"IPY_MODEL_2063dae8b53b427e9cf36863002f3ba7","value":" 684/684 [00:00&lt;00:00, 24.1kB/s]"}},"fcd570f697fd41008a15580aeea7cea3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
